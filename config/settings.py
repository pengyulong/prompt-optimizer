"""
é…ç½®ç®¡ç†æ¨¡å—
"""

import os
from typing import Dict, Any, List, Optional
from dotenv import load_dotenv
from enum import Enum

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from core.models import ModelProvider

class Environment(Enum):
    """ç¯å¢ƒæšä¸¾"""
    DEVELOPMENT = "development"
    PRODUCTION = "production"
    TESTING = "testing"

class AppConfig:
    """åº”ç”¨é…ç½®"""
    
    # åŸºç¡€é…ç½®
    APP_NAME = "æç¤ºè¯ä¼˜åŒ–å™¨"
    VERSION = "2.0.0"
    DESCRIPTION = "AIæç¤ºè¯ä¼˜åŒ–å’Œæµ‹è¯•å·¥å…· - æ”¯æŒå¤šç§AIæ¨¡å‹"
    
    # ç¯å¢ƒé…ç½®
    ENV = Environment(os.getenv("APP_ENV", "development"))
    DEBUG = os.getenv("DEBUG_MODE", "false").lower() == "true"
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
    MAX_CONCURRENT_REQUESTS = int(os.getenv("MAX_CONCURRENT_REQUESTS", "5"))

class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    
    # æ¨¡å‹å®šä¹‰
    MODELS = {
        ModelProvider.OLLAMA: {
            "display_name": "Ollama (æœ¬åœ°)",
            "description": "æœ¬åœ°éƒ¨ç½²çš„å¼€æºæ¨¡å‹",
            "models": [
                {
                    "name": "qwen2.5:latest",
                    "display_name": "é€šä¹‰åƒé—® 2.5 (æœ€æ–°)",
                    "description": "é˜¿é‡Œå·´å·´çš„ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹",
                    "category": "ä¸­æ–‡ä¼˜åŒ–",
                    "context_length": 32768,
                    "parameters": {
                        "temperature": 0.8,
                        "top_p": 0.9,
                        "top_k": 50,
                        "repeat_penalty": 1.05,
                        "max_tokens": 8192
                    }
                }
            ]
        },
        ModelProvider.OPENAI: {
            "display_name": "OpenAI",
            "description": "OpenAI å®˜æ–¹æ¨¡å‹",
            "models": [
                {
                    "name": "gpt-4o",
                    "display_name": "GPT-4o",
                    "description": "OpenAIæœ€æ–°çš„å¤šæ¨¡æ€æ¨¡å‹",
                    "category": "é€šç”¨å¯¹è¯",
                    "context_length": 128000,
                    "parameters": {
                        "temperature": 0.7,
                        "top_p": 1.0,
                        "max_tokens": 4096,
                        "frequency_penalty": 0,
                        "presence_penalty": 0
                    }
                },
                {
                    "name": "gpt-4-turbo",
                    "display_name": "GPT-4 Turbo",
                    "description": "æ›´å¿«é€Ÿçš„GPT-4ç‰ˆæœ¬",
                    "category": "é€šç”¨å¯¹è¯",
                    "context_length": 128000,
                    "parameters": {
                        "temperature": 0.7,
                        "top_p": 1.0,
                        "max_tokens": 4096,
                        "frequency_penalty": 0,
                        "presence_penalty": 0
                    }
                },
                {
                    "name": "gpt-3.5-turbo",
                    "display_name": "GPT-3.5 Turbo",
                    "description": "ç»æµå®ç”¨çš„å¯¹è¯æ¨¡å‹",
                    "category": "é€šç”¨å¯¹è¯",
                    "context_length": 16385,
                    "parameters": {
                        "temperature": 0.7,
                        "top_p": 1.0,
                        "max_tokens": 4096,
                        "frequency_penalty": 0,
                        "presence_penalty": 0
                    }
                }
            ]
        },
        ModelProvider.ANTHROPIC: {
            "display_name": "Anthropic Claude",
            "description": "Anthropic å®˜æ–¹æ¨¡å‹",
            "models": [
                {
                    "name": "claude-3-5-sonnet-20241022",
                    "display_name": "Claude 3.5 Sonnet",
                    "description": "Anthropicæœ€æ–°çš„é«˜æ€§èƒ½æ¨¡å‹",
                    "category": "é€šç”¨å¯¹è¯",
                    "context_length": 200000,
                    "parameters": {
                        "temperature": 0.7,
                        "top_p": 1.0,
                        "max_tokens": 4096
                    }
                },
                {
                    "name": "claude-3-opus-20240229",
                    "display_name": "Claude 3 Opus",
                    "description": "Anthropicæœ€å¼ºå¤§çš„æ¨¡å‹",
                    "category": "é€šç”¨å¯¹è¯",
                    "context_length": 200000,
                    "parameters": {
                        "temperature": 0.7,
                        "top_p": 1.0,
                        "max_tokens": 4096
                    }
                }
            ]
        },
        ModelProvider.DEEPSEEK: {
            "display_name": "DeepSeek",
            "description": "æ·±åº¦æ±‚ç´¢æ¨¡å‹",
            "models": [
                {
                    "name": "deepseek-chat",
                    "display_name": "DeepSeek Chat",
                    "description": "æ·±åº¦æ±‚ç´¢çš„é€šç”¨å¯¹è¯æ¨¡å‹",
                    "category": "é€šç”¨å¯¹è¯",
                    "context_length": 32768,
                    "parameters": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 4096,
                        "frequency_penalty": 0,
                        "presence_penalty": 0
                    }
                },
                {
                    "name": "deepseek-reasoner",
                    "display_name": "DeepSeekæ¨ç†æ¨¡å‹",
                    "description": "æ·±åº¦æ±‚ç´¢çš„æ¨ç†æ¨¡å‹",
                    "category": "æ¨ç†ç±»æ¨¡å‹",
                    "context_length": 32768,
                    "parameters": {
                        "temperature": 0.1,
                        "top_p": 0.95,
                        "max_tokens": 16384,
                        "frequency_penalty": 0,
                        "presence_penalty": 0
                    }
                }
            ]
        },
        ModelProvider.VLLM: {
            "display_name": "vLLM (æœ¬åœ°)",
            "description": "æœ¬åœ°éƒ¨ç½²çš„vLLMæ¨ç†å¼•æ“",
            "models": [
                {
                    "name": "qwen-7b-chat",
                    "display_name": "Qwen-7B-Chat",
                    "description": "é€šä¹‰åƒé—®7Bå¯¹è¯æ¨¡å‹",
                    "category": "é€šç”¨å¯¹è¯",
                    "context_length": 32768,
                    "parameters": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 4096,
                        "frequency_penalty": 0,
                        "presence_penalty": 0
                    }
                },
                {
                    "name": "llama-3-8b-instruct",
                    "display_name": "Llama-3-8B-Instruct",
                    "description": "Meta Llama 3 8BæŒ‡ä»¤æ¨¡å‹",
                    "category": "é€šç”¨å¯¹è¯",
                    "context_length": 8192,
                    "parameters": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 4096,
                        "frequency_penalty": 0,
                        "presence_penalty": 0
                    }
                },
                {
                    "name": "chatglm3-6b",
                    "display_name": "ChatGLM3-6B",
                    "description": "æ™ºè°±AI ChatGLM3 6Bæ¨¡å‹",
                    "category": "ä¸­æ–‡ä¼˜åŒ–",
                    "context_length": 8192,
                    "parameters": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 4096,
                        "frequency_penalty": 0,
                        "presence_penalty": 0
                    }
                }
            ]
        }
    }

class APIConfig:
    """APIé…ç½®"""
    
    CONFIGS = {
        ModelProvider.OLLAMA: {
            "base_url": os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
            "timeout": int(os.getenv("OLLAMA_TIMEOUT", "60")),
            "api_key": None
        },
        ModelProvider.OPENAI: {
            "base_url": os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"),
            "api_key": os.getenv("OPENAI_API_KEY"),
            "timeout": int(os.getenv("OPENAI_TIMEOUT", "60"))
        },
        ModelProvider.ANTHROPIC: {
            "base_url": os.getenv("ANTHROPIC_BASE_URL", "https://api.anthropic.com"),
            "api_key": os.getenv("ANTHROPIC_API_KEY"),
            "timeout": int(os.getenv("ANTHROPIC_TIMEOUT", "60"))
        },
        ModelProvider.QWEN: {
            "base_url": os.getenv("QWEN_BASE_URL", "https://dashscope.aliyuncs.com/api/v1"),
            "api_key": os.getenv("QWEN_API_KEY"),
            "timeout": int(os.getenv("QWEN_TIMEOUT", "60"))
        },
        ModelProvider.CHATGLM: {
            "base_url": os.getenv("CHATGLM_BASE_URL", "https://open.bigmodel.cn/api/paas/v4"),
            "api_key": os.getenv("CHATGLM_API_KEY"),
            "timeout": int(os.getenv("CHATGLM_TIMEOUT", "60"))
        },
        ModelProvider.DEEPSEEK: {
            "base_url": os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com"),
            "api_key": os.getenv("DEEPSEEK_API_KEY"),
            "timeout": int(os.getenv("DEEPSEEK_TIMEOUT", "60"))
        },
        ModelProvider.MOONSHOT: {
            "base_url": os.getenv("MOONSHOT_BASE_URL", "https://api.moonshot.cn/v1"),
            "api_key": os.getenv("MOONSHOT_API_KEY"),
            "timeout": int(os.getenv("MOONSHOT_TIMEOUT", "60"))
        },
        ModelProvider.QIANFAN: {
            "base_url": os.getenv("QIANFAN_BASE_URL", "https://qianfan.baidubce.com/v2"),
            "api_key": os.getenv("QIANFAN_API_KEY", ""),
            "timeout": int(os.getenv("ERNIE_TIMEOUT", "60"))
        },
        ModelProvider.VLLM: {
            "base_url": os.getenv("VLLM_BASE_URL", "http://localhost:8000/v1"),
            "api_key": os.getenv("VLLM_API_KEY", ""),
            "timeout": int(os.getenv("VLLM_TIMEOUT", "60"))
        }
    }

class OptimizationConfig:
    """ä¼˜åŒ–é…ç½®"""
    
    OPTIMIZATION_TYPES = [
        {
            "name": "é€šç”¨ä¼˜åŒ–",
            "key": "general",
            "description": "é€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯çš„é€šç”¨ä¼˜åŒ–ç­–ç•¥",
            "icon": "ğŸ”§"
        },
        {
            "name": "ç»“æ„åŒ–ä¼˜åŒ–",
            "key": "structured", 
            "description": "å¢åŠ æ˜ç¡®çš„ç»“æ„å’Œæ ¼å¼è¦æ±‚",
            "icon": "ğŸ“‹"
        },
        {
            "name": "è§’è‰²å¯¼å‘ä¼˜åŒ–",
            "key": "role_based",
            "description": "åŸºäºç‰¹å®šè§’è‰²æˆ–èº«ä»½è¿›è¡Œä¼˜åŒ–",
            "icon": "ğŸ­"
        },
        {
            "name": "ä»»åŠ¡å¯¼å‘ä¼˜åŒ–",
            "key": "task_oriented",
            "description": "é’ˆå¯¹ç‰¹å®šä»»åŠ¡ç›®æ ‡è¿›è¡Œä¼˜åŒ–",
            "icon": "ğŸ¯"
        },
        {
            "name": "åˆ›æ„ä¼˜åŒ–",
            "key": "creative",
            "description": "æå‡åˆ›æ„æ€§å’Œæƒ³è±¡åŠ›çš„ä¼˜åŒ–",
            "icon": "ğŸ’¡"
        },
        {
            "name": "é€»è¾‘ä¼˜åŒ–",
            "key": "logical",
            "description": "å¢å¼ºé€»è¾‘æ¨ç†å’Œåˆ†æèƒ½åŠ›",
            "icon": "ğŸ§ "
        }
    ]
    
    MAX_PROMPT_LENGTH = 10000
    MAX_OUTPUT_LENGTH = 20000
    DEFAULT_PROVIDER = ModelProvider.DEEPSEEK
    DEFAULT_MODEL = "deepseek-chat"

class TestConfig:
    """æµ‹è¯•é…ç½®"""
    
    METRICS = [
        {
            "name": "accuracy",
            "display_name": "å‡†ç¡®æ€§",
            "description": "å›ç­”çš„å‡†ç¡®æ€§å’Œæ­£ç¡®æ€§",
            "icon": "ğŸ¯"
        },
        {
            "name": "relevance",
            "display_name": "ç›¸å…³æ€§",
            "description": "å›ç­”ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦",
            "icon": "ğŸ”—"
        },
        {
            "name": "completeness",
            "display_name": "å®Œæ•´æ€§",
            "description": "å›ç­”çš„å®Œæ•´æ€§å’Œå…¨é¢æ€§",
            "icon": "ğŸ“"
        },
        {
            "name": "clarity",
            "display_name": "æ¸…æ™°åº¦",
            "description": "å›ç­”çš„æ¸…æ™°åº¦å’Œæ˜“ç†è§£æ€§",
            "icon": "ğŸ’"
        },
        {
            "name": "creativity",
            "display_name": "åˆ›æ„æ€§",
            "description": "å›ç­”çš„åˆ›æ–°æ€§å’Œç‹¬ç‰¹æ€§",
            "icon": "âœ¨"
        }
    ]
    
    MAX_TEST_LENGTH = 5000
    ENABLE_BATCH_TESTING = True
    SHOW_DETAILED_METRICS = True
    SHOW_TOKEN_USAGE = True

class UIConfig:
    """UIé…ç½®"""
    
    THEME = {
        "primary_color": "#FF6B6B",
        "background_color": "#F8F9FA",
        "text_color": "#2C3E50",
        "success_color": "#27AE60",
        "error_color": "#E74C3C",
        "warning_color": "#F39C12",
        "info_color": "#3498DB"
    }
    
    LAYOUT = {
        "sidebar_width": 350,
        "main_content_width": 1000,
        "max_width": 1400
    }
    
    FEATURES = {
        "show_model_status": True,
        "show_connection_status": True,
        "enable_dark_mode": False,
        "show_advanced_options": True,
        "auto_save_history": True
    }

class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    
    ENABLE_CACHE = os.getenv("ENABLE_CACHE", "true").lower() == "true"
    CACHE_TTL = int(os.getenv("CACHE_TTL", "3600"))
    MAX_CACHE_SIZE = 100
    REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
    CACHE_PREFIX = "prompt_optimizer:"

class LogConfig:
    """æ—¥å¿—é…ç½®"""
    
    LEVEL = os.getenv("LOG_LEVEL", "INFO")
    FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    FILE_PATH = "logs/app.log"
    MAX_FILE_SIZE = "10MB"
    BACKUP_COUNT = 5
    LOG_API_CALLS = True
    LOG_USER_ACTIONS = True

class DatabaseConfig:
    """æ•°æ®åº“é…ç½®"""
    
    URL = os.getenv("DATABASE_URL", "sqlite:///./app.db")
    ECHO = AppConfig.DEBUG
    POOL_SIZE = 10
    MAX_OVERFLOW = 20

# é…ç½®éªŒè¯å’Œå·¥å…·å‡½æ•°
class ConfigValidator:
    """é…ç½®éªŒè¯å™¨"""
    
    @staticmethod
    def get_available_providers(require_api_key: bool = True) -> List[ModelProvider]:
        """è·å–å¯ç”¨çš„æ¨¡å‹æä¾›å•†
        Args:
            require_api_key: æ˜¯å¦è¦æ±‚å¿…é¡»é…ç½®API Keyï¼ˆOllamaé™¤å¤–ï¼‰
        """
        available = []
        
        for provider in ModelProvider:
            config = APIConfig.CONFIGS.get(provider, {})
            
            if provider == ModelProvider.OLLAMA:
                # Ollama åªéœ€è¦ base_url
                if config.get("base_url"):
                    available.append(provider)
            else:
                # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦æ£€æŸ¥API Key
                if not require_api_key or config.get("api_key"):
                    available.append(provider)
        
        return available
    
    @staticmethod
    def get_model_config(provider: ModelProvider, model_name: str) -> Optional[Dict[str, Any]]:
        """è·å–æŒ‡å®šæ¨¡å‹çš„é…ç½®"""
        provider_config = ModelConfig.MODELS.get(provider, {})
        models = provider_config.get("models", [])
        
        for model in models:
            if model["name"] == model_name:
                return model
        
        return None
    
    @staticmethod
    def get_api_config(provider: ModelProvider) -> Optional[Dict[str, Any]]:
        """è·å–æŒ‡å®šæä¾›å•†çš„APIé…ç½®"""
        return APIConfig.CONFIGS.get(provider)
    
    @staticmethod
    def validate_config() -> Dict[str, Any]:
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        issues = []
        warnings = []
        
        # æ£€æŸ¥å¯ç”¨æä¾›å•†
        available_providers = ConfigValidator.get_available_providers()
        if not available_providers:
            issues.append("æ²¡æœ‰é…ç½®ä»»ä½•å¯ç”¨çš„æ¨¡å‹æä¾›å•†")
        
        # æ£€æŸ¥é»˜è®¤æä¾›å•†
        if OptimizationConfig.DEFAULT_PROVIDER not in available_providers:
            warnings.append(f"é»˜è®¤æä¾›å•† '{OptimizationConfig.DEFAULT_PROVIDER.value}' ä¸å¯ç”¨")
        
        # æ£€æŸ¥æ—¥å¿—ç›®å½•
        log_dir = os.path.dirname(LogConfig.FILE_PATH)
        if log_dir and not os.path.exists(log_dir):
            try:
                os.makedirs(log_dir, exist_ok=True)
            except Exception as e:
                warnings.append(f"æ— æ³•åˆ›å»ºæ—¥å¿—ç›®å½•: {e}")
        
        return {
            "valid": len(issues) == 0,
            "issues": issues,
            "warnings": warnings,
            "available_providers": [p.value for p in available_providers]
        }

# æ¨¡å—åŠ è½½æ—¶éªŒè¯é…ç½®
_validation_result = ConfigValidator.validate_config()

# é…ç½®éªŒè¯ç»“æœå·²é™é»˜å¤„ç†

# é…ç½®é»˜è®¤æ¨¡å‹å’Œæ¨¡å‹æä¾›å•†
DEFAULT_PROVIDER = ModelProvider.DEEPSEEK
DEFAULT_MODEL = "deepseek-chat"

# å¯¼å‡ºä¸»è¦é…ç½®ç±»
__all__ = [
    "AppConfig",
    "ModelConfig", 
    "APIConfig",
    "OptimizationConfig",
    "TestConfig",
    "UIConfig",
    "CacheConfig",
    "LogConfig",
    "DatabaseConfig",
    "ConfigValidator",
    "ModelProvider",
    "Environment"
]