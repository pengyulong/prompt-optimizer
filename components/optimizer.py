"""
æç¤ºè¯ä¼˜åŒ–ç»„ä»¶
"""

import streamlit as st
from typing import Optional, Dict, Any
import time

from core.models import (
    OptimizationRequest, OptimizationType, ModelProvider, GenerationConfig
)
from services.optimization import OptimizationService
from config.settings import OptimizationConfig, UIConfig
from utils.logger import get_logger, user_logger
from utils.exceptions import ModelError, ValidationError

logger = get_logger(__name__)

class OptimizerComponent:
    """æç¤ºè¯ä¼˜åŒ–ç»„ä»¶"""
    
    def __init__(self):
        self.optimization_service = OptimizationService()
    
    def render(self):
        """æ¸²æŸ“ä¼˜åŒ–ç»„ä»¶"""
        try:
            self._render_input_section()
            self._render_config_section()
            self._render_optimization_button()
            self._render_results_section()
        except Exception as e:
            logger.error(f"æ¸²æŸ“ä¼˜åŒ–ç»„ä»¶å¤±è´¥: {str(e)}")
            st.error(f"ç»„ä»¶æ¸²æŸ“å¤±è´¥: {str(e)}")
    
    def _render_input_section(self):
        """æ¸²æŸ“è¾“å…¥åŒºåŸŸ"""
        st.subheader("ğŸ“ è¾“å…¥æç¤ºè¯")
        
        # åŸå§‹æç¤ºè¯è¾“å…¥
        original_prompt = st.text_area(
            "è¯·è¾“å…¥éœ€è¦ä¼˜åŒ–çš„åŸå§‹æç¤ºè¯",
            height=150,
            key="original_prompt",
            placeholder="åœ¨æ­¤è¾“å…¥æ‚¨çš„åŸå§‹æç¤ºè¯...\n\nä¾‹å¦‚ï¼š\n- è¯·ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€\n- å¸®æˆ‘å†™ä¸€ä¸ªæ’åºç®—æ³•\n- åˆ†æè¿™ç¯‡æ–‡ç« çš„ä¸»è¦è§‚ç‚¹",
            help="è¾“å…¥æ‚¨æƒ³è¦ä¼˜åŒ–çš„æç¤ºè¯ï¼Œæ”¯æŒå¤šè¡Œæ–‡æœ¬"
        )
        
        # è¾“å…¥éªŒè¯
        if original_prompt:
            char_count = len(original_prompt)
            max_length = OptimizationConfig.MAX_PROMPT_LENGTH
            
            # æ˜¾ç¤ºå­—ç¬¦è®¡æ•°
            if char_count > max_length:
                st.error(f"âš ï¸ æç¤ºè¯é•¿åº¦è¶…å‡ºé™åˆ¶: {char_count}/{max_length} å­—ç¬¦")
            else:
                progress = char_count / max_length
                st.progress(progress, text=f"å­—ç¬¦æ•°: {char_count}/{max_length}")
        
        return original_prompt
    
    def _render_config_section(self):
        """æ¸²æŸ“é…ç½®åŒºåŸŸ"""
        # st.subheader("âš™ï¸ ä¼˜åŒ–é…ç½®")
        
        # è·å–é»˜è®¤æ¨¡å‹é…ç½®
        selected_model = self._get_default_model()
        
        # ä¼˜åŒ–ç­–ç•¥é€‰æ‹©
        optimization_options = OptimizationConfig.OPTIMIZATION_TYPES
        opt_display = [f"{opt['icon']} {opt['name']}" for opt in optimization_options]
        
        # ç”Ÿæˆå”¯ä¸€keyï¼ŒåŒ…å«ç»„ä»¶åç§°ã€ä¼šè¯IDå’Œå½“å‰æ—¶é—´æˆ³
        import uuid
        strategy_key = f"optimizer_strategy_select_{st.session_state.get('session_id','default')}_{uuid.uuid4().hex}"
        selected_opt_index = st.selectbox(
            "ä¼˜åŒ–ç­–ç•¥",
            range(len(opt_display)),
            format_func=lambda x: opt_display[x],
            key=strategy_key,
            help="é€‰æ‹©é€‚åˆæ‚¨éœ€æ±‚çš„ä¼˜åŒ–ç­–ç•¥"
        )
            
        selected_opt = optimization_options[selected_opt_index]
            
        # æ˜¾ç¤ºç­–ç•¥æè¿°
        st.info(f"ğŸ’¡ {selected_opt['description']}")
        
        # é«˜çº§é€‰é¡¹
        if st.checkbox("ğŸ”§ é«˜çº§é€‰é¡¹", key="show_advanced_options"):
            self._render_advanced_options()
        
        return selected_model, selected_opt, self._get_generation_config()
    
    def _render_advanced_options(self):
        """æ¸²æŸ“é«˜çº§é€‰é¡¹"""
        # st.markdown("**ç”Ÿæˆå‚æ•°è°ƒæ•´**")
        
        col1, col2 = st.columns(2)
        
        with col1:
            temperature = st.slider(
                "åˆ›é€ æ€§ (Temperature)",
                min_value=0.0,
                max_value=2.0,
                value=0.7,
                step=0.1,
                key="temperature",
                help="è¾ƒé«˜å€¼ä½¿è¾“å‡ºæ›´éšæœºï¼Œè¾ƒä½å€¼ä½¿è¾“å‡ºæ›´ç¡®å®š"
            )
        
        with col2:
            top_p = st.slider(
                "æ ¸é‡‡æ · (Top-p)",
                min_value=0.1,
                max_value=1.0,
                value=0.9,
                step=0.1,
                key="top_p",
                help="æ§åˆ¶è¯æ±‡å¤šæ ·æ€§çš„å‚æ•°"
            )
        
        # ç³»ç»Ÿæç¤ºè¯
        system_prompt = st.text_area(
            "ç³»ç»Ÿæç¤ºè¯ (å¯é€‰)",
            key="system_prompt",
            placeholder="ä¸ºAIè®¾å®šè§’è‰²æˆ–è¡Œä¸ºè§„èŒƒ...",
            help="å¯ä»¥ä¸ºAIè®¾å®šç‰¹å®šçš„è§’è‰²æˆ–è¡Œä¸ºå‡†åˆ™"
        )
    
    def _render_optimization_button(self):
        """æ¸²æŸ“ä¼˜åŒ–æŒ‰é’®"""
        original_prompt = st.session_state.get("original_prompt", "")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¾“å…¥
        if not original_prompt.strip():
            st.warning("âš ï¸ è¯·å…ˆè¾“å…¥éœ€è¦ä¼˜åŒ–çš„æç¤ºè¯")
            return
        
        # æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
        if not self._check_model_availability():
            st.error("âŒ æ‰€é€‰æ¨¡å‹ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–é€‰æ‹©å…¶ä»–æ¨¡å‹")
            return
        
        if st.button("ğŸš€ å¼€å§‹ä¼˜åŒ–", type="primary", use_container_width=True, key="optimize_button"):
            self._execute_optimization()
    
    def _execute_optimization(self):
        """æ‰§è¡Œä¼˜åŒ–"""
        try:
            original_prompt = st.session_state.get("original_prompt", "")
            
            # è·å–é…ç½®
            model_info, opt_type, gen_config = self._render_config_section()
            
            if not all([model_info, opt_type, gen_config]):
                st.error("é…ç½®ä¿¡æ¯ä¸å®Œæ•´")
                return
            
            # åˆ›å»ºä¼˜åŒ–è¯·æ±‚
            request = OptimizationRequest(
                original_prompt=original_prompt,
                optimization_type=OptimizationType(opt_type["key"]),
                model_provider=ModelProvider(model_info["provider"]),
                model_name=model_info["name"],
                generation_config=gen_config,
                user_id=st.session_state.get("session_id")
            )
            
            # æ˜¾ç¤ºè¿›åº¦
            with st.spinner("ğŸ”„ æ­£åœ¨ä¼˜åŒ–ä¸­ï¼Œè¯·ç¨å€™..."):
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # æ›´æ–°è¿›åº¦
                progress_bar.progress(0.3)
                status_text.text("æ­£åœ¨åˆ†æåŸå§‹æç¤ºè¯...")
                time.sleep(0.5)
                
                progress_bar.progress(0.6)
                status_text.text("æ­£åœ¨ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
                
                # æ‰§è¡Œä¼˜åŒ–
                result = self.optimization_service.optimize_prompt(request)
                
                progress_bar.progress(1.0)
                status_text.text("ä¼˜åŒ–å®Œæˆï¼")
                time.sleep(0.5)
                
                # æ¸…é™¤è¿›åº¦æ˜¾ç¤º
                progress_bar.empty()
                status_text.empty()
            
            if result and result.response.success:
                # ä¿å­˜ç»“æœåˆ°session state
                st.session_state.current_optimization = result
                
                # æ·»åŠ åˆ°ç”¨æˆ·ä¼šè¯å†å²
                st.session_state.user_session.add_optimization(result)
                
                # è®°å½•ç”¨æˆ·æ“ä½œ
                user_logger.log_optimization(
                    user_id=st.session_state.get("session_id"),
                    optimization_type=opt_type["name"],
                    model=f"{model_info['provider']}/{model_info['name']}",
                    prompt_length=len(original_prompt),
                    success=True
                )
                
                # æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
                st.success(f"âœ… ä¼˜åŒ–å®Œæˆï¼ç”¨æ—¶ {result.response.response_time:.2f} ç§’")
                
                # è‡ªåŠ¨æ»šåŠ¨åˆ°ç»“æœåŒºåŸŸ
                st.rerun()
                
            else:
                error_msg = result.response.error if result else "æœªçŸ¥é”™è¯¯"
                st.error(f"âŒ ä¼˜åŒ–å¤±è´¥: {error_msg}")
                
                # è®°å½•å¤±è´¥çš„æ“ä½œ
                user_logger.log_optimization(
                    user_id=st.session_state.get("session_id"),
                    optimization_type=opt_type["name"],
                    model=f"{model_info['provider']}/{model_info['name']}",
                    prompt_length=len(original_prompt),
                    success=False
                )
                
        except ValidationError as e:
            st.error(f"âŒ è¾“å…¥éªŒè¯å¤±è´¥: {str(e)}")
        except ModelError as e:
            st.error(f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")
        except Exception as e:
            logger.error(f"ä¼˜åŒ–æ‰§è¡Œå¤±è´¥: {str(e)}")
            st.error(f"âŒ ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    def _render_results_section(self):
        """æ¸²æŸ“ç»“æœåŒºåŸŸ"""
        st.subheader("ğŸ“‹ ä¼˜åŒ–ç»“æœ")
        
        if "current_optimization" not in st.session_state or not st.session_state.current_optimization:
            st.info("ğŸ’¡ ä¼˜åŒ–åçš„æç¤ºè¯å°†åœ¨æ­¤æ˜¾ç¤º...")
            return
        
        result = st.session_state.current_optimization
        original_prompt = st.session_state.get("original_prompt", "")
        
        # åˆ›å»ºæ ‡ç­¾é¡µ
        tab1, tab2, tab3 = st.tabs(["ğŸ“‹ å¯¹æ¯”ç»“æœ", "ğŸ’¡ ä¼˜åŒ–å»ºè®®", "ğŸ“Š è¯¦ç»†ä¿¡æ¯"])
        
        with tab1:
            self._render_comparison_tab(result, original_prompt)
        
        with tab2:
            self._render_suggestions_tab(result)
        
        with tab3:
            self._render_details_tab(result)
    
    def _render_comparison_tab(self, result, original_prompt):
        """ç»Ÿä¸€å¸ƒå±€çš„å¯¹æ¯”æ ‡ç­¾é¡µ"""
        # ä½¿ç”¨å‚ç›´å¸ƒå±€ä»£æ›¿æ°´å¹³å¸ƒå±€
        st.markdown("####  ğŸ“ åŸå§‹æç¤ºè¯")
        st.text_area(
            "åŸå§‹æç¤ºè¯",
            value=original_prompt,
            height=200,
            key="original_display",
            disabled=True,
            label_visibility="collapsed"
        )
        
        # ä¼˜åŒ–ç»“æœå±•ç¤º
        st.markdown("#### ğŸ”§ ä¼˜åŒ–åæç¤ºè¯")
        st.text_area(
            "ä¼˜åŒ–åçš„æç¤ºè¯",
            value=result.optimized_prompt,
            height=200,
            key="optimized_display",
            label_visibility="collapsed"
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        col1, col2 = st.columns(2)
        original_length = len(original_prompt)
        optimized_length = len(result.optimized_prompt)
        length_diff = optimized_length - original_length
        
        with col1:
            st.metric("åŸå§‹é•¿åº¦", f"{original_length} å­—ç¬¦")
        
        with col2:
            st.metric("ä¼˜åŒ–åé•¿åº¦", f"{optimized_length} å­—ç¬¦", 
                    delta=f"{'+' if length_diff > 0 else ''}{length_diff} å­—ç¬¦")
        
        # å¤åˆ¶æŒ‰é’®ç»„
        copy_cols = st.columns(2)
        with copy_cols[0]:
            if st.button("ğŸ“‹ å¤åˆ¶åŸå§‹æç¤ºè¯", use_container_width=True):
                st.session_state.copied_text = original_prompt
                st.toast("å·²å¤åˆ¶åŸå§‹æç¤ºè¯", icon="ğŸ“‹")
        
        with copy_cols[1]:
            if st.button("ğŸ“‹ å¤åˆ¶ä¼˜åŒ–åæç¤ºè¯", type="primary", use_container_width=True):
                st.session_state.copied_text = result.optimized_prompt
                st.toast("å·²å¤åˆ¶ä¼˜åŒ–åæç¤ºè¯", icon="ğŸ“‹")
    
    def _render_suggestions_tab(self, result):
        """æ¸²æŸ“å»ºè®®æ ‡ç­¾é¡µ"""
        st.markdown("**ğŸ¯ ä¼˜åŒ–è¦ç‚¹**")
        
        for i, suggestion in enumerate(result.suggestions, 1):
            st.markdown(f"{i}. {suggestion}")
        
        # ä¼˜åŒ–ç±»å‹è¯´æ˜
        st.markdown("---")
        st.markdown(f"**ğŸ“‹ ä½¿ç”¨çš„ä¼˜åŒ–ç­–ç•¥**: {result.request.optimization_type.value}")
        
        # æ¨¡å‹ä¿¡æ¯
        st.markdown(f"**ğŸ¤– ä½¿ç”¨çš„æ¨¡å‹**: {result.request.model_provider.value} / {result.request.model_name}")
        
        # æ€§èƒ½æŒ‡æ ‡
        if result.metrics:
            st.markdown("**ğŸ“Š æ€§èƒ½æŒ‡æ ‡**")
            cols = st.columns(len(result.metrics))
            for i, (metric, value) in enumerate(result.metrics.items()):
                with cols[i]:
                    st.metric(metric.title(), f"{value:.2f}")
    
    def _render_details_tab(self, result):
        """æ¸²æŸ“è¯¦ç»†ä¿¡æ¯æ ‡ç­¾é¡µ"""
        st.markdown("**â±ï¸ æ‰§è¡Œä¿¡æ¯**")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("å“åº”æ—¶é—´", f"{result.response.response_time:.2f}s")
        
        with col2:
            if result.response.tokens_used:
                st.metric("ç”ŸæˆTokens", result.response.tokens_used)
            else:
                st.metric("ç”ŸæˆTokens", "N/A")
        
        with col3:
            if result.response.tokens_prompt:
                st.metric("è¾“å…¥Tokens", result.response.tokens_prompt)
            else:
                st.metric("è¾“å…¥Tokens", "N/A")
        
        with col4:
            total_tokens = 0
            if result.response.tokens_used and result.response.tokens_prompt:
                total_tokens = result.response.tokens_used + result.response.tokens_prompt
            st.metric("æ€»Tokens", total_tokens if total_tokens > 0 else "N/A")
        
        # å…ƒæ•°æ®ä¿¡æ¯
        if result.response.metadata:
            st.markdown("**ğŸ” å…ƒæ•°æ®**")
            
            # æ ¼å¼åŒ–æ˜¾ç¤ºå…ƒæ•°æ®
            metadata_display = {}
            for key, value in result.response.metadata.items():
                if isinstance(value, (int, float)):
                    if 'duration' in key.lower():
                        # è½¬æ¢çº³ç§’ä¸ºæ¯«ç§’
                        if value > 1000000:
                            metadata_display[key] = f"{value / 1000000:.2f} ms"
                        else:
                            metadata_display[key] = f"{value:.2f} ns"
                    else:
                        metadata_display[key] = str(value)
                else:
                    metadata_display[key] = str(value)
            
            for key, value in metadata_display.items():
                st.text(f"{key}: {value}")
        
        # æ—¶é—´æˆ³
        st.markdown("---")
        st.caption(f"åˆ›å»ºæ—¶é—´: {result.created_at.strftime('%Y-%m-%d %H:%M:%S')}")
    
    def _get_default_model(self) -> Dict[str, str]:
        """è·å–é»˜è®¤æ¨¡å‹é…ç½®"""
        return {
            "provider": OptimizationConfig.DEFAULT_PROVIDER.value,
            "name": OptimizationConfig.DEFAULT_MODEL,
            "display_name": OptimizationConfig.DEFAULT_MODEL
        }
    
    def _check_model_availability(self):
        """æ£€æŸ¥æ‰€é€‰æ¨¡å‹å¯ç”¨æ€§"""
        try:
            if not st.session_state.model_client:
                return False
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„æ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥é€»è¾‘
            return True
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§å¤±è´¥: {str(e)}")
            return False
    
    def _get_generation_config(self) -> GenerationConfig:
        """è·å–ç”Ÿæˆé…ç½®"""
        config = GenerationConfig()
        
        # å¦‚æœå¯ç”¨äº†é«˜çº§é€‰é¡¹ï¼Œä½¿ç”¨ç”¨æˆ·è®¾ç½®
        if st.session_state.get("show_advanced_options", False):
            config.temperature = st.session_state.get("temperature", 0.7)
            config.max_tokens = st.session_state.get("max_tokens", 4096)
            config.top_p = st.session_state.get("top_p", 0.9)
            config.system_prompt = st.session_state.get("system_prompt", "")
        
        return config

# å¯¼å‡ºç»„ä»¶ç±»
__all__ = ["OptimizerComponent"]